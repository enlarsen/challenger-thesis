\chapter{Literature Review}\label{litrev}

This chapter reviews efficient market, asset pricing, and event study literature in order to provide an overview of the theory. The first area addressed will be relevant efficient markets literature. Early efficient market literature dealt primarily with what are called random walks, or the random behavior of security prices, until work by Fama showed that random walks are a certain form of efficient markets. Prior to Fama's work, there was some work that studied the underlying distribution of security prices. That is, it answered the question of whether security prices follow a normal distribution. This led to discoveries of what we will call here ``depth of efficiency,'' namely that the interesting question with regard to efficiency is not whether exists. Rather, the interesting question is to what degree or depth are markets efficient.

The capital asset pricing model (CAPM) is then covered along with its relationship to efficient markets through the joint-hypothesis problem. The joint-hypothesis problem appears in studies of efficient markets because these studies require that two hypotheses be tested. First, efficiency is tested in the study (generally what is wanted), and, second, an asset pricing model is required to define efficiency. Thus, tests of efficiency are not performed without this model of capital assets and therefore cannot test purely for the effects of efficient markets.

Next, event study literature is discussed. The current literature points out the problems of event studies, which are statistical techniques for determining whether a firm or portfolio realized an abnormal return for the date in question (termed an ``event''). Event studies are based on Fama's early work in different forms of market efficiency, particularly in what he called semi-strong form efficiency. Later, in fact, Fama changed the naming of semi-strong form efficiency to ``event studies.''

\section{Efficient Markets}

At the turn of the century Louis Bachelier \cite{bachelier} wrote his doctoral dissertation in mathematics on the movement of stock prices. His findings were that stocks moved in a random manner, a random walk.

This finding meant that there were no opportunities that could be exploited in order to reap profits; if stocks moved at random, there was no way of devising what is termed a {\em trading rule} to make abnormal returns, or, here, profits. The random movement of stock prices, on the surface, had a number of implications. The finding of randomness was the main implication, meaning that stock prices exhibited no ``intelligence.'' The finding of randomness precluded the formulation hypotheses about stock price movements. There was evidence that stock prices did not move randomly. A missing link existed somewhere. Stock prices did move randomly, but they also responded to news. Empirical studies showed what appeared to be nonrandom stock price movements, unexplained by the theory. In effect, there were a series of empirical results that were in search of a theory. The random walk hypothesis provided part of the answer, but it was only with the work of Samuelson \cite{samuelson} and Mandelbrot \cite{mandelbrot} that this theory was provided; Samuelson and Mandelbrot found the link between the random walk hypothesis and efficient markets. An efficient market is one which adjusts instantaneously to new information, denying a trader the opportunity of making abnormal profits. If there were opportunities to be exploited (that is, a trading rule exists) efficiency requires that these opportunities be instantly exploited until the marginal benefit of exploiting it (the abnormal returns) was bid away until it equaled the marginal cost. The theory that was tested was that no abnormal returns existed in capital markets.

%One explanation was the random walk hypothesis, that prices merely moved about as a stochastic process. The other was that markets were efficient and adjusted instantly to incorporate all information.

Samuelson and Mandelbrot found that we would expect prices to move randomly under an efficient market regime, but that is only because the market quickly (almost instantaneously) incorporates information into the price of securities. They refined the random walk theory to show it is a special case of efficient markets. The securities market, in adjusting so rapidly to new information, appears only to be a random walk. A true random walk model, as will be shown later, would be indifferent to news or new information arriving about the underlying value of the firms. The securities market is thus a clearinghouse for information. This observation leads to some interesting possibilities for statistical analysis of stock prices (actually returns) to determine the impact of new information. This statistical analysis is called an ``event study'' and will be discussed later.

Fama \cite{fama70} expanded on Samuelson's and Mandelbrot's research, defining different tests of efficiency. Fama was not concerned whether the market was efficient. Rather, he concerned himself with to what degree or depth the market was efficient. He first clarified the theory of efficient markets. In doing so he modeled efficiency formally as,

\begin{equation}
x_{j,t+1}=p_{j,t+1}-E(p_{j,t+1} \mid \Phi_{t}).
\end{equation}

In this equation $p_{j,t+1}$ is the price of security $j$ at time $t+1$. $E$ is the expectations operator, and $\Phi_{t}$ is the information held by the markets. This equation represents what is termed a ``fair game'' with respect to the information set $\Phi_{t}$ when

\begin{equation}
E(\tilde x_{j,t+1} \mid \Phi_{t})=0.
\end{equation}

Definitionally, $x_{j,t+1}$ refers to an abnormal price. When $x_{j,t+1}=0$ we find a fair game. That is, the expected future price is equal to the future price; there is no systematic bias in expected returns. If there were a difference between the left hand side and the right hand side, we might expect to find the possibility for abnormal returns.

Fama continues the same discussion using returns instead of prices. He defines,

\begin{equation}
z_{j,t+1}=r_{j,t+1}-E(\tilde r_{j,t+1} \mid \Phi_{t}),
\end{equation}

and a ``fair game'' (with respect to the information set $\Phi_{t}$) as

\begin{equation}
E(\tilde z_{j,t+1} \mid \Phi_{t}) = 0.
\end{equation}

In these equations $r_{j,t+1}$ is defined as a ``return'' or a percentage change over two time periods $t$ and $t+1$ of security $j$'s price. Or $r_{j,t+1} = (p_{j,t+1}-p_{j,t})/p_{j,t}$. The distinction between prices and returns becomes more important later due to problems using prices with event studies instead of returns.

\section{Submartingales and Random Walks}

Fama then discusses two special cases of efficiency,
submartingales and random walks. He defines a submartingale
formally
as

\begin{equation}
E(\tilde p_{j,t+1} \mid \Phi_{t}) \geq p_{j,t}.
\end{equation}

which is equivalent to

\begin{equation}
E(\tilde r_{j,t+1} \mid \Phi_{t}) \geq 0.
\end{equation}

This says that the expected value of next period's price (return) is greater than or equal to the current price (return). In other words, the price is expected to rise. Tomorrow's price (return) is expected to rise.

% Younger me wrote this: 
%\footnote{This is an error in Fama's original equation, as presented here. The equations should both be {\em strictly} greater. Otherwise a submartingale could be nothing more than ordinary efficiency (if each period's price is {\em equal} to the expected next period price, as would be the case if equality held).}
% Older me deleted it.

Fama points out the empirical implications of a submartingale. Namely, if tomorrow's price is expected to rise, then a study of stock prices must take this into consideration. A test of abnormal returns must compare the portfolio's return to a strategy of buying and holding securities (which, following a submartingale, are expected to rise). Without this consideration, a study would be biased upward.

Finally, Fama discusses formally the random walk model. The random walk model was what Bachelier used to model security prices at the turn of the century. Fama defines random walks as a special case of efficient markets. His model of random walks consists of two hypotheses. First, successive price changes are independent and second they are identically distributed (or they are i.i.d.). Fama defines these conditions formally as

\begin{equation}
f(r_{j,t+1} \mid \Phi_{t})=f(r_{j,t+1}).
\end{equation}

This defines the equality between tomorrow's returns conditional on information set $\Phi_{t}$ (with $f$ as a probability density function) and tomorrow's returns without the information set. More simply, the probability density functions are the same with and without the information set. This is a strong requirement; it requires that the distributions both have the same mean, variance, skewness, and kurtosis regardless of the information set. A random walk thus has the requirement that not only that the difference between expected future returns (or prices) and future returns must equal zero, but also that the distribution of returns is the same whether or not a set of information $\Phi_{t}$ is used. This additional requirement of equivalent distributions makes a random walk a special case of the efficient market.

Fama addresses the issue of sufficient conditions for market efficiency, conditions where a security's price fully reflects all available information. There are three. First, there are no transaction costs. Second, all available information is costlessly available to all market participants. Third, all participants agree on the implications of the information on a security's future prices. That is, there is no disagreement about how an announcement (dividend change, positive NPV project, accident) will affect a company's health.

Fama continues by describing tests of efficient markets. He divides the tests into three increasing levels. Each level represents a progressively ``deeper'' incorporation of information into the market. These tests are often later referred to not as {\em tests} of market efficiency, but as different degrees of market efficiency. For instance, some authors refer to weak-form efficiency rather than ``weak-form {\em tests} of efficiency.'' Fama actually referred to the tests, not varying degrees of efficiency.

The first test, called a weak-form test of efficiency, tests whether the securities market is efficient with respect to historical prices. A market that is efficient in terms of past security prices (no trading rule can be devised based on past stock price information and technical analysis is ineffective) could be called {\em weak-form efficient}. Traders have known that historical information affects securities in a certain manner and have bid away all of the profits associated with this knowledge.

The second test, which we will be concerned with here, is called a semi-strong form test of efficiency. Semi-strong form tests of efficiency show that once information becomes public, the market will react, removing any opportunity for profit from this information. A market that exhibits this characteristic is called ``strong-form efficient.'' Strong form efficiency is ``deeper'' than weak-form in the sense that any publicly available information is incorporated into the current price. This encompasses any publicly available information, but also includes historical returns (a weak-form efficiency requirement) to determine future prices. A trader cannot make abnormal returns by using information such as earnings and investment reports. Efficiency in the semi-strong form requires that as soon as this information becomes public the abnormal returns have been bid away. Semi-strong form tests form the basis of event studies. Since strong-form efficiency requires that the market adjust to take into account information that has just become public, a news item will instantly affect a firms price. This effect can be discerned using statistical techniques, confirming or rejecting a hypothesis a researcher might have about the certain news on companies or an industry.

The third form of market efficiency is called strong-form efficiency. Strong-form efficiency means that any and all information is quickly incorporated into the market price of a security. That is, company directors and executives have inside information on their firm which they use trying to obtain abnormal returns. Strong-form efficiency is not of interest here because the markets are not, in general, strong-form efficient. This notion is reinforced by the existence of laws that outlaw insider trading.

These three forms of efficiency form the basis for empirical studies of market efficiency. Fama \cite{fama91} reviews the current literature in efficient markets. He changes the tests of efficiency from weak form to ``tests of return predictability,'' from semi-strong form to ``event studies,'' and from strong form to ``tests for private information.'' In this paper Fama points to one of the problems with assessing market efficiency. Namely, tests of efficiency must be undertaken with some model of asset pricing. This is most often the capital asset pricing model, or CAPM, which evaluates riskiness of assets as the covariance between the (risky) asset in question and a market portfolio (all assets). This is related to one of the characteristics of efficient markets that Fama pointed out in his earlier work \cite{fama70}: ``all agree on the implications of current information for the current price and distributions of future prices of each security.'' This assumption is that all traders operate under the same model of efficiency in the capital markets.

This is what is termed the joint hypothesis problem. Fama states the problem this way, ``as a result, when we find anomalous evidence on the behavior of returns, the way it should be split between market inefficiency or a bad model of market equilibrium is ambiguous.'' \cite{fama91} But this does not mean that tests of efficiency are not useful; on the contrary, the vast literature spawned by this research and the changes on the investment community cannot be understated. The end result, however, is that they symmetry between tests of efficiency and an asset pricing model will make it impossible to know precisely how efficiency works.

\section{Capital Asset Pricing}

The joint-hypothesis problem requires that we address the issue of equilibrium models of markets as they are necessary for any tests of capital market efficiency. An equilibrium model of capital markets provides a framework for understanding why there is no excess demand in the securities markets. One of the first to devise such a framework was William Sharpe. His paper ``Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk'' \cite{sharpe64} lays out what would later become known as the capital asset pricing model (CAPM). CAPM (or the security market line) will not be derived here, but some of the salient points will be covered.

CAPM provides a model of asset pricing using a linear function of the risk-free rate. The risk-free rate is the rate on return that prevails on riskless assets. CAPM uses this risk-free rate to determine the return and variance of an efficient portfolio. A given portfolio is seen as a tradeoff between risk and return. A curve traced out showing this tradeoff is called the minimum variance opportunity set. By drawing a line from the risk free rate to the upper portion of the minimum variance opportunity set, one can determine the optimal portfolio. This linear function, called the security market line, captures the price of risk as its slope; the slope is called beta.

Beta is defined as the covariance with the market divided by the variance of the market, or \cite[p. 198]{copeland},

\begin{equation}
\beta_{i}=\frac{\sigma_{im}}{\sigma_{m}^{2}}=\frac{cov(R_{i},R_{m})}{var(R_{m})}.
\end{equation}

The beta of the market portfolio is defined as,

\begin{equation}
\beta_{m}=\frac{cov(R_{m},R_{m})}{var(R_{m})}=\frac{var(R_{m})}{var(R_{m})}=1.
\end{equation}

Thus, $\beta_{i}$ gives an indication of the relationship between asset $i$'s risk versus the market portfolio's risk. CAPM dictates that these two measures should move together, unless the underlying riskiness of the asset changes. It is from this model that we can formulate event studies. Event studies become a matter of regressing the return of asset $i$ on the market portfolio, $R_{m}$.

%CAPM from Copeland and Weston \cite[p. 194]{copeland}
%
%\begin{singlespace}
%\begin{enumerate}
%\item Investors are risk-averse individuals who maximize the expected utility of their end-of-period wealth.
%\item Investors are price takers and have homogeneous expectations about asset returns that have a joint normal distribution.
%\item There exists a risk-free asset such that investors may borrow or lend unlimited amounts at the risk-free rate.
%\item The quantities of assets are fixed. Also, all assets are marketable and perfectly divisible.
%\item Asset markets are frictionless and information is costless an simultaneously available to all investors.
%\item There are no market imperfections such as taxes, regulations, or restrictions on short selling.
%\end{enumerate}
%\end{singlespace}

\section{Event Studies}

The second form of efficiency (called semi-strong form) has spawned a vast literature in what are termed event studies. Event studies are a statistical technique of measuring the abnormal performance on a securities return.\footnote{Returns are defined as

\begin{equation}
R_{j,t+1}=\frac{P_{j,t+1}-P_{j,t}}{P_{j,t}},
\end{equation}

where $R_{j,t}$ is defined as the (one period) return for time $t$, $P_{j,t}$ is security $j$'s price at time period $t$, and $P_{j,t+1}$ is security $j$'s price at time $t+1$.} An event study allows researchers to test for market efficiency firstly, but also enables the study of firm related issues. For instance, an event study would allow a researcher to test hypotheses of how the stock market views dividends or how the market views different capital structures.
 
%The first event study was performed by Fama, Fisher, Jensen, and Roll \cite{famaetal69}, assessing the effect of dividend and split announcements on the price of shares. This paper derived a number of techniques that continue to be used in event study research.

The event study is based on a statistical model of security prices. Current event studies generally use a simple linear function to model abnormal returns. That is,

\begin{eqnarray}
R_{it}=\alpha_i+\beta_i R_{mt}+\varepsilon_{it}, \\
\varepsilon_{it} \sim N(0,\sigma^{2}),
\end{eqnarray}

where $R_{it}$ is the return of security $i$ at time $t$, $R_{mt}$ is the market return at time $t$, $\varepsilon_{it}$ is the error term, and $\alpha_i$ and $\beta_i$ are estimated parameters. The error term, $\varepsilon_{it}$ is assumed to be distributed normally with mean zero and variance $\sigma^{2}$. This is a simple linear regression of a security's returns on the market returns. The residuals are measures of abnormal performance. According to CAPM, the security's return movements should be a function of the market's returns. When there are large residuals (verified by t-tests) the researcher fails to reject the hypothesis that that the event had no effect on the value of the firm.

When an event study is performed where an actual date is at issue, a different model is often used,

\begin{equation}
R_{it}=\alpha_i+\beta_i R_{mt}+\gamma_i D_{i} + \varepsilon_{it}.
\end{equation}

$D_{i}$ is a dummy variable is set to one for the date (or dates) of the event and zero otherwise. This has two purposes. The first is to allow the statistical software to calculate a t-test of the coefficient's significance. The other is an {\em a priori} assumption that the event is significant and thus an outlier. If it were not ``dummied out'' it would bias the estimation of the coefficient on $\beta$.

When the date of the event is not so clear-cut, or the information reached the market over time, it is often useful not to use the event dummy variable and simply analyze the abnormal returns (or residuals). Fama, Jensen, and Roll \cite{famaetal69} devise a technique of studying the error terms (the $\varepsilon_{it}$'s). One measure they use is called the cumulative average residual, defined as

\begin{equation}
U_m=\sum_{k=1}^{m}\frac{\sum_{j=1}^{N_k}\hat u_{jk}}{N_{k}}.
\end{equation}

This is simply the sum of the portfolio average residuals summed from the beginning of the period until the current time period. It is a running sum of the average portfolio residual. This is valuable (assuming that beta remains constant) for showing graphically how a stock's abnormal return moved versus the market. Under normal efficient conditions, the cumulative average residual should move around zero, not unlike white noise.

\subsection{Nonsynchronicity of Daily Returns}

With the arrival of the Center for Research in Security Prices (CRSP) tapes has come the ability to easily study daily data. Unfortunately, using daily data presents the researcher with some problems, which a number of papers address. Two papers are discussed here that investigate the nonsynchronous nature of daily data. The nonsynchronous nature of stock returns stems from the uneven spacing of daily prices (which are used to compute returns) and because often smaller stocks do not trade every trading day. These are called unobservable returns. Scholes and Williams, state, ``errors in variables result when measured returns are used as proxies for true unobservable returns'' \cite[p. 311]{scholes77}. They say that, ``with daily data this problem [of nonsynchronous daily data] appears particularly severe'' \cite[p. 309]{scholes77}. Since securities are traded at discrete, stochastic intervals of time, Scholes and Williams argue there are errors in variables, finding that OLS estimates of $\alpha$ and $\beta$ are in error. They provide a method for obtaining consistent estimators.

Dimson \cite{dimson79} devises a simpler technique for obtaining a true estimate of beta, which he calls the ``aggregated coefficients (AC) method.'' Dimson represents this as,

\begin{equation}
\hat R_t=\hat \alpha+ \sum_{k=-n}^{n} \hat \beta_k \hat M_{t+k}+w_{t}.
\end{equation}

The regression equation thus contains a series of terms for the market return. That is, it contains leading, lagged, and matched values for the market return. Dimson claims this market equation returns the true systematic risk. Thus, ``\ldots the true systematic risk \ldots can be obtained from security price data which is subject to infrequent trading\ldots'' \cite[p. 204]{dimson79} as,

\begin{equation}
\hat \beta = \sum_{k=-n}^{n} \hat \beta_{k}.
\end{equation}

Brown and Warner \cite{brownwarner83} conduct a simulation study to study the various event study methodologies. The data studied are from the CRSP tapes and consist of firms and dates chosen at random. In some studies they perturb the event date by adding or subtracting some amount from the return for that day. This allows them to study the various aspects of event studies including nonsynchronous trading, event window, event clustering, small samples, and power of the tests. The finding of interest here is nonsynchronous trading problems. They find that using techniques other than OLS ``\ldots convey no clear-cut benefit in detecting abnormal performance'' \cite[p. 26]{brownwarner83}.

\subsection{Beta Shift}

Another problem that appears is a shift in beta after the event. A shift in beta indicates the firm (or portfolio) in question realized a change in riskiness. This can be remedied (in fact, it is often specifically tested for in studies of the effect of regulation on industries) by using this market model,

\begin{equation}
R_{t}=\alpha+\beta R_{m} + \delta D_{t} + \gamma D_{t} R_{m} + \varepsilon.
\end{equation}

The dummy variable $D_{t}$ takes on one or zero depending on how the systematic risk test is structured. For some studies, $D_{t}$ is set to one during the event period and zero otherwise to test whether systematic risk changed during the event period. Other studies (such as regulation studies) set $D_{t}$ to one after the event (and zero otherwise) to determine whether the event shifted the beta thereafter.\footnote{In regulation studies a shift in beta would indicate a new state of the world and if this new state of the world affects the company in a significant way (indicated by a change in the riskiness of the shares). For example, regulation that grants monopoly-like privileges to the company would be hypothesized to lower beta (lower risk) after the regulatory event.}

\subsection{Non-Stationarity of Beta}

Another problem is non-stationarity of beta. Beta is assumed to be stable over the event period, but it actually changes over time. Beta changes should represent white noise; they should be nonsystematic. If these changes are systematic such as beta increasing over time, the event study will be biased. One way to test for this is compute moving average beta over time (beta is computed using a window that shifts forward one day at a time). A beta is computed for one period of time. Subsequent betas are computed by adding one trading day to the regression and deleting the last. For instance, if the estimate of beta extend from $t-n$ to $t+n$ trading days, the next estimate of beta would extend from $t-n+1$ to $t+n+1$ trading days. The first order differences of these betas is then computed and checked for correspondence with white noise (using the runs test or correlation between successive differences). A first order difference is, for example, $\tilde \beta_{t+1} - \tilde \beta_{t},$ where $\tilde \beta_{t}$ represents the current beta and $\tilde \beta_{t+1}$ represents the beta calculated from shifting the window forward one day.

\section{Summary}

This chapter tied together some of the disparate literature and theory on security prices, the capital asset pricing model, and capital market efficiency. Stock prices have been studied for many years until a model of their movement was discovered---called a random walk. This model was not entirely satisfactory and turned out to be a special case of efficient markets. After Fama devised various degrees of market efficiency and performed what appears to be the first event study, the use of statistical methods to calculate compliance of semi-strong form efficiency to real-world data as been a growth industry. The problem of the joint hypothesis remains, but it is not a barrier to discovering important findings in studies of efficient markets.

Event studies themselves have been the subject of much study, and the arrival of the CRSP tapes has allowed researchers to study abnormal performance in depth using daily returns. Daily returns present some problems, the most important of which is nonsynchronous trading. However, actual empirical simulations by Brown and Warner \cite{brownwarner83} showed that the nonsynchronous trading problem is not as grave as was believed by others such as Dimson \cite{dimson79} or Scholes and Williams \cite{scholes77}.
